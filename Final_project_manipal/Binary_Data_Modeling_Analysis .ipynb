{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\.conda\\envs\\tensorflow\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, learning_curve\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the dateset into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_users_df = pd.read_pickle('existing_users_2classes_df.pickle')\n",
    "existing_users_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "seed = 42\n",
    "df = shuffle(existing_users_df, random_state=seed)\n",
    "df0= df[df.next_purchase_day_2class==0].iloc[:100000]\n",
    "df1 = df[df.next_purchase_day_2class==1].iloc[:100000]\n",
    "df = pd.concat([df1,df0])\n",
    "df.groupby('next_purchase_day_2class')['next_purchase_day'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into training 80% and test data 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"next_purchase_day_2class\"] = df[\"next_purchase_day_2class\"].astype(int)\n",
    "y = df[\"next_purchase_day_2class\"].values\n",
    "X = df.drop(labels = [\"next_purchase_day_2class\",\"user_id\",\"next_purchase_day\"],axis = 1)# Create Train & Test Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "X,y = shuffle(X,y, random_state=seed)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Cross Validation with multiple models and scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code was borrowed from an opensource repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "def print_results(names, results, test_scores):\n",
    "    print()\n",
    "    print(\"#\" * 30 + \"Results\" + \"#\" * 30)\n",
    "    counter = 0\n",
    "\n",
    "    class Color:\n",
    "        PURPLE = '\\033[95m'\n",
    "        CYAN = '\\033[96m'\n",
    "        DARKCYAN = '\\033[36m'\n",
    "        BLUE = '\\033[94m'\n",
    "        GREEN = '\\033[92m'\n",
    "        YELLOW = '\\033[93m'\n",
    "        RED = '\\033[91m'\n",
    "        BOLD = '\\033[1m'\n",
    "        UNDERLINE = '\\033[4m'\n",
    "        END = '\\033[0m'\n",
    "\n",
    "    # Get max row\n",
    "    clf_names = set([name.split(\"_\")[1] for name in names])\n",
    "    max_mean = {name: 0 for name in clf_names}\n",
    "    max_mean_counter = {name: 0 for name in clf_names}\n",
    "    for name, result in zip(names, results):\n",
    "        counter += 1\n",
    "        clf_name = name.split(\"_\")[1]\n",
    "        if result.mean() > max_mean[clf_name]:\n",
    "            max_mean_counter[clf_name] = counter\n",
    "            max_mean[clf_name] = result.mean()\n",
    "\n",
    "    # print max row in BOLD\n",
    "    counter = 0\n",
    "    prev_clf_name = names[0].split(\"_\")[1]\n",
    "    for name, result, score in zip(names, results, test_scores):\n",
    "        counter += 1\n",
    "        clf_name = name.split(\"_\")[1]\n",
    "        if prev_clf_name != clf_name:\n",
    "            print()\n",
    "            prev_clf_name = clf_name\n",
    "        msg = \"%s: %f (%f) [test_score:%.3f]\" % (name, result.mean(), result.std(), score)\n",
    "        if counter == max_mean_counter[clf_name]:\n",
    "            print(Color.BOLD + msg)\n",
    "        else:\n",
    "            print(Color.END + msg)\n",
    "\n",
    "\n",
    "def create_pipelines(seed, verbose=1):\n",
    "    \"\"\"\n",
    "         Creates a list of pipelines with preprocessing(PCA), models and scalers.\n",
    "\n",
    "    :param seed: Random seed for models who needs it\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    models = [\n",
    "              ('LR', LogisticRegression(solver='lbfgs',multi_class ='multinomial')),\n",
    "              ('LDA', LinearDiscriminantAnalysis()),\n",
    "              ('KNN', KNeighborsClassifier()),\n",
    "              ('CART', DecisionTreeClassifier(random_state=seed)),\n",
    "              ('NB', GaussianNB()),\n",
    "              ('SVM', SVC(random_state=seed, probability=True)),\n",
    "              ('LinearSVC',LinearSVC(max_iter=100,dual=False)),\n",
    "              ('RF', RandomForestClassifier(max_depth=3, random_state=seed)),\n",
    "              ('XGB', XGBClassifier(max_depth=5, learning_rate=0.08, objective= 'multi:softmax',n_jobs=-1,num_class=3)),\n",
    "              ('Bagging',BaggingClassifier()),\n",
    "              ('ExtraTrees',ExtraTreesClassifier(n_estimators=100))\n",
    "        \n",
    "              ]\n",
    "    scalers = [('StandardScaler', StandardScaler()),\n",
    "               ('MinMaxScaler', MinMaxScaler()),\n",
    "               ('MaxAbsScaler', MaxAbsScaler()),\n",
    "               ('RobustScaler', RobustScaler()),\n",
    "               ('QuantileTransformer-Normal', QuantileTransformer(output_distribution='normal')),\n",
    "               ('QuantileTransformer-Uniform', QuantileTransformer(output_distribution='uniform')),\n",
    "               ('PowerTransformer-Yeo-Johnson', PowerTransformer(method='yeo-johnson')),\n",
    "               ('Normalizer', Normalizer())\n",
    "               ]\n",
    "    #additions = [('PCA', PCA(n_components=4)),\n",
    "                # ]\n",
    "    # Create pipelines\n",
    "    pipelines = []\n",
    "    for model in models:\n",
    "        # Append only model\n",
    "        model_name = \"_\" + model[0]\n",
    "        pipelines.append((model_name, Pipeline([model])))\n",
    "\n",
    "        # Append model+scaler\n",
    "        for scalar in scalers:\n",
    "            model_name = scalar[0] + \"_\" + model[0]\n",
    "            pipelines.append((model_name, Pipeline([scalar, model])))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Created these pipelines:\")\n",
    "        for pipe in pipelines:\n",
    "            print(pipe[0])\n",
    "\n",
    "    return pipelines\n",
    "\n",
    "\n",
    "def run_cv_and_test(X_train, y_train, X_test, y_test, pipelines, scoring, seed, num_folds,\n",
    "                    dataset_name, n_jobs):\n",
    "    \"\"\"\n",
    "\n",
    "        Iterate over the pipelines, calculate CV mean and std scores, fit on train and predict on test.\n",
    "        Return the results in a dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # List that contains the rows for a dataframe\n",
    "    rows_list = []\n",
    "\n",
    "    # Lists for the pipeline results\n",
    "    results = []\n",
    "    names = []\n",
    "    test_scores = []\n",
    "    prev_clf_name = pipelines[0][0].split(\"_\")[1]\n",
    "    print(\"First name is : \", prev_clf_name)\n",
    "\n",
    "    for name, model in pipelines:\n",
    "        kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)\n",
    "        cv_results = ms.cross_val_score(model, X_train, y_train, cv=kfold, n_jobs=n_jobs, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "\n",
    "        # Print CV results of the best CV classier\n",
    "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "\n",
    "        # fit on train and predict on test\n",
    "        model.fit(X_train, y_train)\n",
    "        if scoring == \"accuracy\":\n",
    "            curr_test_score = model.score(X_test, y_test)\n",
    "        elif scoring == \"roc_auc\":\n",
    "            y_pred = model.predict_proba(X_test)[:, 1]\n",
    "            curr_test_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        test_scores.append(curr_test_score)\n",
    "\n",
    "        # Add separation line if different classifier applied\n",
    "        rows_list, prev_clf_name = check_seperation_line(name, prev_clf_name, rows_list)\n",
    "\n",
    "        # Add for final dataframe\n",
    "        results_dict = {\"Dataset\": dataset_name,\n",
    "                        \"Classifier_Name\": name,\n",
    "                        \"CV_mean\": cv_results.mean(),\n",
    "                        \"CV_std\": cv_results.std(),\n",
    "                        \"Test_score\": curr_test_score\n",
    "                        }\n",
    "        rows_list.append(results_dict)\n",
    "\n",
    "    print_results(names, results, test_scores)\n",
    "\n",
    "    df = pd.DataFrame(rows_list)\n",
    "    return df[[\"Dataset\", \"Classifier_Name\", \"CV_mean\", \"CV_std\", \"Test_score\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "import numpy as np\n",
    "\n",
    "# Global_vars\n",
    "seed = 42\n",
    "num_folds = 12\n",
    "n_jobs = -1\n",
    "hypertuned_experiment = True\n",
    "is_save_results = True\n",
    "# Create pipelines\n",
    "pipelines = create_pipelines(seed)\n",
    "scoring = \"accuracy\"\n",
    "results_df = run_cv_and_test(X_train, y_train, X_test, y_test, pipelines, scoring, seed, num_folds,\n",
    "                                     dataset_name=\"existing user\", n_jobs=n_jobs)\n",
    "\n",
    "        # Save cv experiment to csv\n",
    "if is_save_results:\n",
    "    dataset_results_name = \"existing_user_results-2classes.csv\"\n",
    "    results_path = os.path.join(dataset_results_name)\n",
    "    results_df.to_csv(results_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pdresults_file\n",
    "results_file= \"existing_user_results-3classes.csv\"\n",
    "results_df = pd.read_csv(os.path.join(results_file)).dropna().round(3)\n",
    "import operator\n",
    "results_df.loc[operator.and_(results_df[\"Classifier_Name\"].str.startswith(\"_\"), ~results_df[\"Classifier_Name\"].str.endswith(\"PCA\"))].dropna()\n",
    "temp = results_df.loc[~results_df[\"Classifier_Name\"].str.endswith(\"PCA\")].dropna()\n",
    "temp[\"model\"] = results_df[\"Classifier_Name\"].apply(lambda sen: sen.split(\"_\")[1])\n",
    "temp[\"scaler\"] = results_df[\"Classifier_Name\"].apply(lambda sen: sen.split(\"_\")[0])\n",
    "def df_style(val):\n",
    "    return 'font-weight: 800'\n",
    "pivot_t = pd.pivot_table(temp, values='CV_mean', index=[\"scaler\"], columns=['model'], aggfunc=np.sum)\n",
    "pivot_t_bold = pivot_t.style.applymap(df_style,\n",
    "                      subset=pd.IndexSlice[pivot_t[\"CART\"].idxmax(),\"CART\"])\n",
    "for col in list(pivot_t):\n",
    "    pivot_t_bold = pivot_t_bold.applymap(df_style,\n",
    "                      subset=pd.IndexSlice[pivot_t[col].idxmax(),col])\n",
    "pivot_t_bold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_max_vals = {}\n",
    "cols_max_row_names = {}\n",
    "for col in list(pivot_t):\n",
    "    row_name = pivot_t[col].idxmax()\n",
    "    cell_val = pivot_t[col].max()\n",
    "    cols_max_vals[col] = cell_val\n",
    "    cols_max_row_names[col] = row_name\n",
    "    \n",
    "sorted_cols_max_vals = sorted(cols_max_vals.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(\"Best classifiers sorted:\\n\")\n",
    "counter = 1\n",
    "for model, score in sorted_cols_max_vals:\n",
    "    print(str(counter) + \". \" + model + \" + \" +cols_max_row_names[model] + \" : \" +str(score))\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Hyperparameter Tuning for performance improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypertune_params():\n",
    "    \"\"\"\n",
    "\n",
    "        Create a dictionary with classifier name as a key and it's hyper parameters options as a value\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # RF PARAMS\n",
    "    n_estimators = [int(x) for x in np.linspace(start=3, stop=20, num=3)]\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    rf_params = {'RF__n_estimators': n_estimators,\n",
    "                 'RF__max_features': max_features,\n",
    "                 'RF__max_depth': max_depth,\n",
    "                 'RF__min_samples_split': min_samples_split,\n",
    "                 'RF__min_samples_leaf': min_samples_leaf,\n",
    "                 }\n",
    "\n",
    "\n",
    "    # SVM PARAMS\n",
    "    C = [x for x in np.arange(0.1, 2, 0.2)]\n",
    "    kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "    svm_params = {'SVM__C': C,\n",
    "                  'SVM__kernel': kernel,\n",
    "                  }\n",
    "\n",
    "    # Logistic Regression Params\n",
    "    C = [x for x in np.arange(0.1, 3, 0.2)]\n",
    "    penalty = [\"l2\"]\n",
    "    fit_intercept = [True, False]\n",
    "    lr_params = {'LR__C': C,\n",
    "                 'LR__penalty': penalty,\n",
    "                 'LR__fit_intercept': fit_intercept\n",
    "                 }\n",
    "\n",
    "    # LDA PARAMS\n",
    "    solver = [\"lsqr\"]\n",
    "    shrinkage = [\"auto\", None, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    lda_params = {'LDA__solver': solver,\n",
    "                  'LDA__shrinkage': shrinkage\n",
    "                  }\n",
    "\n",
    "    hypertuned_params = {\"RF\": rf_params,\n",
    "                         \"SVM\": svm_params,\n",
    "                         \"LR\": lr_params,\n",
    "                         }\n",
    "\n",
    "    return hypertuned_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def run_cv_and_test_hypertuned_params(X_train, y_train, X_test, y_test, pipelines, scoring, seed, num_folds,\n",
    "                                      dataset_name, hypertuned_params, n_jobs):\n",
    "    \"\"\"\n",
    "\n",
    "        Iterate over the pipelines, calculate CV mean and std scores, fit on train and predict on test.\n",
    "        Return the results in a dataframe\n",
    "\n",
    "    :param X_train:\n",
    "    :param y_train:\n",
    "    :param X_test:\n",
    "    :param y_test:\n",
    "    :param scoring:\n",
    "    :param seed:\n",
    "    :param num_folds:\n",
    "    :param dataset_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # List that contains the rows for a dataframe\n",
    "    rows_list = []\n",
    "\n",
    "    # Lists for the pipeline results\n",
    "    results = []\n",
    "    names = []\n",
    "    test_scores = []\n",
    "    prev_clf_name = pipelines[0][0].split(\"_\")[1]\n",
    "    print(\"First name is : \", prev_clf_name)\n",
    "\n",
    "    # To be used within GridSearch (5 in your case)\n",
    "    inner_cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    # To be used in outer CV (you asked for num_folds)\n",
    "    outer_cv = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "    for name, model in pipelines:\n",
    "\n",
    "        # Get model's hyper parameters\n",
    "        model_name = name.split(\"_\")[1]\n",
    "        if \"-\" in model_name:\n",
    "            model_name = model_name.split(\"-\")[0]\n",
    "\n",
    "        if model_name in hypertuned_params.keys():\n",
    "            random_grid = hypertuned_params[model_name]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Train nested-CV\n",
    "        clf = GridSearchCV(estimator=model, param_grid=random_grid, cv=inner_cv, scoring=scoring,\n",
    "                           verbose=2, n_jobs=n_jobs, refit=True)\n",
    "        cv_results = model_selection.cross_val_score(clf, X_train, y_train, cv=outer_cv, n_jobs=n_jobs, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "\n",
    "        # Print CV results of the best CV classier\n",
    "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "\n",
    "        # fit on train and predict on test\n",
    "        model.fit(X_train, y_train)\n",
    "        if scoring is \"accuracy\":\n",
    "            curr_test_score = model.score(X_test, y_test)\n",
    "        elif scoring is \"roc_auc\":\n",
    "            y_pred = model.predict(X_test)\n",
    "            curr_test_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        test_scores.append(curr_test_score)\n",
    "\n",
    "        # Add separation line if different classifier applied\n",
    "        rows_list, prev_clf_name = check_seperation_line(name, prev_clf_name, rows_list)\n",
    "\n",
    "        # Add for final dataframe\n",
    "        results_dict = {\"Dataset\": dataset_name,\n",
    "                        \"Classifier_Name\": name,\n",
    "                        \"CV_mean\": cv_results.mean(),\n",
    "                        \"CV_std\": cv_results.std(),\n",
    "                        \"Test_score\": curr_test_score\n",
    "                        }\n",
    "        rows_list.append(results_dict)\n",
    "\n",
    "    print_results(names, results, test_scores)\n",
    "\n",
    "    df = pd.DataFrame(rows_list)\n",
    "    return df[[\"Dataset\", \"Classifier_Name\", \"CV_mean\", \"CV_std\", \"Test_score\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_hyp_save_results = True\n",
    "# Run same experiment with hypertuned parameters\n",
    "print(\"#\"*30 + \"Hyper tuning parameters\" \"#\"*30)\n",
    "hypertuned_params = get_hypertune_params()\n",
    "\n",
    "hypertune_results_df = run_cv_and_test_hypertuned_params(X_train, y_train, X_test, y_test, pipelines, scoring, seed,\n",
    "                                                         num_folds, dataset_name=\"existing user\", n_jobs=n_jobs,\n",
    "                                                         hypertuned_params=hypertuned_params,)\n",
    "\n",
    "if is_hyp_save_results:\n",
    "    dataset_results_name = \"existing_user_results_hypertuned-2classes.csv\"\n",
    "    results_path = os.path.join(dataset_results_name)\n",
    "    hypertune_results_df.to_csv(results_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection as ms\n",
    "#classifiers = [['LR',LogisticRegression()],['KNN',KNeighborsClassifier(5)],['SVC', SVC(kernel=\"linear\", C=0.025)], ['RandForest',RandomForestClassifier(max_depth=5)],['Boost',AdaBoostClassifier()], ['Gaussian',GaussianNB()]]\n",
    "classifiers = [['LR',LogisticRegression(solver='lbfgs', C=0.1)]]\n",
    "kfold = ms.KFold(n_splits=5, random_state=22)\n",
    "for name, model in classifiers:\n",
    "    result = ms.cross_val_score(model, X_train, y_train, cv=kfold).mean()\n",
    "    print(name,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from yellowbrick.datasets import load_occupancy\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "# Load the classification data set\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', C=0.01)\n",
    "viz = FeatureImportances(model, size=(1080, 720))\n",
    "viz.fit(X, y)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "\n",
    "cm = ConfusionMatrix(\n",
    "    model, classes=['<=14 days','>14 days'],\n",
    "    label_encoder={0: '<=14 days', 1: '>14 days'},\n",
    "    size=(1080, 720)\n",
    ")\n",
    "\n",
    "cm.fit(X_train, y_train)\n",
    "cm.score(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "cm.poof()\n",
    "print('Accuracy' + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "\n",
    "visualizer = ClassificationReport(\n",
    "    model, classes=[0,1], support=True, size=(1080, 720)\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.poof()                 # Draw/show/poof the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
